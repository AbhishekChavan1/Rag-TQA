{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6Pen_Df4e47"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers sentence-transformers accelerate bitsandbytes\n",
        "!pip install chromadb langchain langchain_community langchain_huggingface langgraph\n",
        "!pip install --upgrade langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. Imports\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from typing import TypedDict, List, Optional\n",
        "from functools import partial\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langgraph.graph import StateGraph, END\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4lxESOC7neG",
        "outputId": "bccdfb51-c149-40e1-8188-cf7d83441e30"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. DEFINE THE AGENT STATE\n",
        "# ==============================================================================\n",
        "from typing import TypedDict, List, Optional\n",
        "from langchain_core.messages import BaseMessage # Import BaseMessage for typing\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    prompt: str\n",
        "    data_sources: Optional[List[str]]\n",
        "    tools: list\n",
        "    plan: dict\n",
        "    code: Optional[str]\n",
        "    critique: Optional[str]\n",
        "    documentation: Optional[str]\n",
        "    revision_number: int\n",
        "    messages: List[BaseMessage] # Add messages to store conversation history"
      ],
      "metadata": {
        "id": "zqA53mjS8Rsp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. SETUP HUGGING FACE MODELS\n",
        "# ==============================================================================\n",
        "\n",
        "# Define the model ID\n",
        "model_id = \"distilbert/distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load the model without 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    #use_auth_token=None, # Replace with your HF token if using a gated model\n",
        ")\n",
        "\n",
        "# Create a Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=2048,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "# Wrap the pipeline in the LangChain object\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# --- Embedding Model for RAG ---\n",
        "print(\"Loading Embedding Model: BAAI/bge-large-en-v1.5\")\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "# --- Reranker Model for RAG Accuracy ---\n",
        "print(\"Loading Reranker Model: BAAI/bge-reranker-large\")\n",
        "reranker_model = HuggingFaceCrossEncoder(\n",
        "    model_name=\"BAAI/bge-reranker-large\",\n",
        "    model_kwargs={'device': device}\n",
        ")\n",
        "print(\"--- All models loaded successfully ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh0zBthW4i32",
        "outputId": "12213bf5-7f64-4af7-fe9c-1a933aa85f36"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Embedding Model: BAAI/bge-large-en-v1.5\n",
            "Loading Reranker Model: BAAI/bge-reranker-large\n",
            "--- All models loaded successfully ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. DEFINE THE GRAPH NODES\n",
        "# ==============================================================================\n",
        "# --- NODE 1: RAG Setup Node ---\n",
        "def agent_node(state: AgentState, llm_with_tools):\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def tool_executor_node(state: AgentState, tool_executor):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
        "        return\n",
        "    tool_invocations = last_message.tool_calls\n",
        "    tool_outputs = tool_executor.batch(tool_invocations)\n",
        "    tool_messages = [ToolMessage(content=str(output), tool_call_id=inv[\"id\"]) for inv, output in zip(tool_invocations, tool_outputs)]\n",
        "    return {\"messages\": tool_messages}\n",
        "\n",
        "def should_continue(state: AgentState):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
        "        return \"call_tools\"  # Route to the tool executor\n",
        "    return \"end_loop\"\n",
        "\n",
        "def planner_entry_node(state: AgentState):\n",
        "    planner_prompt = f\"\"\"Based on the user's request, create a detailed, structured plan for writing the required code.\n",
        "    Your final output must be a valid JSON object. Use your search tool if you need to find information about libraries or hardware.\n",
        "\n",
        "    User Request: {state['messages'][0].content}\n",
        "    \"\"\"\n",
        "    return {\"messages\": [HumanMessage(content=planner_prompt)]}"
      ],
      "metadata": {
        "id": "n2P2qJdDBD_h"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. DEFINE THE GRAPH NODES\n",
        "# ==============================================================================\n",
        "# --- NODE 1: RAG Setup Node ---\n",
        "\n",
        "def rag_setup_node(state: AgentState):\n",
        "    if not state.get(\"data_sources\"):\n",
        "        print(\"--- RAG SETUP: No data sources. Providing an empty tool list. ---\")\n",
        "        return {\"tools\": []}\n",
        "\n",
        "    persist_directory = \"./chroma_db\"\n",
        "\n",
        "    if os.path.exists(persist_directory):\n",
        "        print(f\"--- RAG SETUP: Loading existing Chroma DB from {persist_directory} ---\")\n",
        "        # Assuming embedding_model is defined globally or in a previous cell\n",
        "        vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
        "    else:\n",
        "        print(f\"--- RAG SETUP: Initializing RAG for sources: {state['data_sources']} ---\")\n",
        "        loader = WebBaseLoader(state[\"data_sources\"])\n",
        "        docs = loader.load()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        splits = text_splitter.split_documents(docs)\n",
        "\n",
        "        # Assuming embedding_model is defined globally or in a previous cell\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=splits,\n",
        "            embedding=embedding_model, # <-- Use the HF model here\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "        vectorstore.persist() # Persist the database\n",
        "\n",
        "    base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    # Assuming reranker_model is defined globally or in a previous cell\n",
        "    compressor = CrossEncoderReranker(model=reranker_model, top_n=3)\n",
        "    compression_retriever = ContextualCompressionRetriever(\n",
        "        base_compressor=compressor, base_retriever=base_retriever\n",
        "    )\n",
        "\n",
        "    rag_tool = create_retriever_tool(\n",
        "        compression_retriever,\n",
        "        \"documentation_search\",\n",
        "        \"Searches technical documentation for hardware specs, library usage, and pin configurations.\"\n",
        "    )\n",
        "\n",
        "    print(\"--- RAG SETUP: Advanced RAG tool with reranking is ready. ---\")\n",
        "    return {\"tools\": [rag_tool]}\n",
        "\n",
        "# --- Generic Agent Prompt Template ---\n",
        "agent_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert agent. Your goal is to achieve the user's request by using your tools and providing a final, comprehensive answer.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"), # Use chat_history for memory\n",
        "        (\"human\", \"{input}\"), # Use input for the current prompt\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- NODE 2: Planner Agent ---\n",
        "def planner_agent_node(state: AgentState):\n",
        "    print(\"--- PLANNER AGENT: Starting... ---\")\n",
        "    planner_prompt = f\"\"\"Based on the user's request, create a detailed, structured plan for writing the required code.\n",
        "    Your final output must be a valid JSON object following this schema:\n",
        "    {{\n",
        "      \"platform\": \"arduino or raspberry_pi\",\n",
        "      \"board\": \"Specific board name (e.g., Arduino Uno, Raspberry Pi Pico)\",\n",
        "      \"language\": \"e.g., C++, MicroPython\",\n",
        "      \"components\": [\"List of hardware components\"],\n",
        "      \"pins\": {{\"component_name\": \"pin_number\"}},\n",
        "      \"logic_steps\": [\"Step-by-step logic\"],\n",
        "      \"required_libraries\": [\"List of libraries\"]\n",
        "    }}\n",
        "\n",
        "    User Request: {state['prompt']}\n",
        "    \"\"\"\n",
        "    # Assuming llm and state[\"tools\"] are defined globally or in previous cells\n",
        "    # Bind tools to the LLM\n",
        "    llm_with_tools = llm.bind_tools(state[\"tools\"])\n",
        "    agent = create_tool_calling_agent(llm_with_tools, state[\"tools\"], agent_prompt)\n",
        "    # Configure AgentExecutor with memory\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=state[\"tools\"], verbose=True, memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True))\n",
        "    result = agent_executor.invoke({\"input\": planner_prompt, \"chat_history\": state.get(\"messages\", [])}) # Pass messages to memory\n",
        "\n",
        "    try:\n",
        "        plan = json.loads(result[\"output\"])\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"--- PLANNER AGENT: Warning: Failed to parse JSON, trying a fallback parser. ---\")\n",
        "        fallback_parser = JsonOutputParser()\n",
        "        plan = fallback_parser.invoke(result[\"output\"])\n",
        "\n",
        "\n",
        "    print(\"--- PLANNER AGENT: Plan generated. ---\")\n",
        "    return {\"plan\": plan, \"messages\": state.get(\"messages\", []) + [HumanMessage(content=planner_prompt), result[\"output\"]]} # Update messages\n",
        "\n",
        "# --- NODE 3: Code Generator Agent ---\n",
        "def code_generator_agent_node(state: AgentState):\n",
        "    print(\"--- CODE GENERATOR AGENT: Starting... ---\")\n",
        "    generator_prompt = f\"\"\"\n",
        "    You are an expert programmer. Generate the complete, runnable code based on the plan.\n",
        "    Only output the raw code inside a single code block (e.g., ```python ... ```). Do not add any other text or explanations.\n",
        "\n",
        "    Plan:\n",
        "    {json.dumps(state[\"plan\"], indent=2)}\n",
        "\n",
        "    {'Critique from previous attempt: ' + state['critique'] if state.get('critique') else ''}\n",
        "    \"\"\"\n",
        "    # Assuming llm and state[\"tools\"] are defined globally or in previous cells\n",
        "    # Bind tools to the LLM\n",
        "    llm_with_tools = llm.bind_tools(state[\"tools\"])\n",
        "    agent = create_tool_calling_agent(llm_with_tools, state[\"tools\"], agent_prompt)\n",
        "    # Configure AgentExecutor with memory\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=state[\"tools\"], verbose=True, memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True))\n",
        "    result = agent_executor.invoke({\"input\": generator_prompt, \"chat_history\": state.get(\"messages\", [])}) # Pass messages to memory\n",
        "\n",
        "\n",
        "    # Clean up the output to extract only the code\n",
        "    code = StrOutputParser().invoke(result[\"output\"]).strip().replace(\"```python\", \"\").replace(\"```c++\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "\n",
        "    print(\"--- CODE GENERATOR AGENT: Code generated. ---\")\n",
        "    return {\"code\": code, \"revision_number\": state.get(\"revision_number\", 0) + 1, \"messages\": state.get(\"messages\", []) + [HumanMessage(content=generator_prompt), result[\"output\"]]} # Update messages\n",
        "\n",
        "# --- NODE 4: Validator Node ---\n",
        "def validator_node(state: AgentState):\n",
        "    print(\"--- VALIDATOR: Reviewing code... ---\")\n",
        "    validator_prompt = f\"\"\"You are a code reviewer. Review the code against the plan.\n",
        "    If the code is correct and follows the plan, respond with the single word \"OK\".\n",
        "    Otherwise, provide a concise, bulleted list of errors and suggestions for fixing them.\n",
        "\n",
        "    Plan: {json.dumps(state['plan'])}\n",
        "    Code:\n",
        "    {state['code']}\n",
        "    \"\"\"\n",
        "    # Assuming llm is defined globally or in a previous cell\n",
        "    critique = llm.invoke(validator_prompt)\n",
        "\n",
        "    print(f\"--- VALIDATOR: Critique: {critique} ---\")\n",
        "    if \"OK\" in critique.upper():\n",
        "        print(\"--- VALIDATOR: Code is good! ---\")\n",
        "        return {\"critique\": None, \"messages\": state.get(\"messages\", []) + [HumanMessage(content=validator_prompt), AIMessage(content=\"OK\")]} # Update messages with OK\n",
        "    else:\n",
        "        return {\"critique\": critique, \"messages\": state.get(\"messages\", []) + [HumanMessage(content=validator_prompt), AIMessage(content=critique)]} # Update messages with critique\n",
        "\n",
        "# --- NODE 5: Documentation Generator ---\n",
        "def documentation_generator_node(state: AgentState):\n",
        "    print(\"--- DOCUMENTATION GENERATOR: Creating documentation... ---\")\n",
        "    doc_prompt = f\"\"\"Generate a README.md for this project. Include sections for:\n",
        "    1. Project Overview\n",
        "    2. Hardware Requirements & Wiring\n",
        "    3. Software Setup\n",
        "    4. Code Explanation\n",
        "\n",
        "    Original Request: {state['prompt']}\n",
        "    Plan: {json.dumps(state['plan'])}\n",
        "    Final Code:\n",
        "    {state['code']}\n",
        "    \"\"\"\n",
        "    # Assuming llm is defined globally or in a previous cell\n",
        "    documentation = llm.invoke(doc_prompt)\n",
        "    print(\"--- DOCUMENTATION GENERATOR: Complete ---\")\n",
        "    return {\"documentation\": documentation, \"messages\": state.get(\"messages\", []) + [HumanMessage(content=doc_prompt), AIMessage(content=documentation)]} # Update messages"
      ],
      "metadata": {
        "id": "Il6p2fdH6-dA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. CONSTRUCT THE GRAPH\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"--- Constructing Agent Graph ---\")\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"rag_setup\", rag_setup_node)\n",
        "workflow.add_node(\"planner_agent\", planner_agent_node)\n",
        "workflow.add_node(\"code_generator_agent\", code_generator_agent_node)\n",
        "workflow.add_node(\"validator\", validator_node)\n",
        "workflow.add_node(\"documentation_generator\", documentation_generator_node)\n",
        "\n",
        "# Define edges\n",
        "workflow.set_entry_point(\"rag_setup\")\n",
        "workflow.add_edge(\"rag_setup\", \"planner_agent\")\n",
        "workflow.add_edge(\"planner_agent\", \"code_generator_agent\")\n",
        "workflow.add_edge(\"code_generator_agent\", \"validator\")\n",
        "workflow.add_edge(\"documentation_generator\", END)\n",
        "\n",
        "# Define conditional logic for the validation loop\n",
        "def should_continue(state: AgentState):\n",
        "    if state[\"critique\"] is None:\n",
        "        return \"generate_docs\"\n",
        "    if state[\"revision_number\"] > 3:\n",
        "        print(\"--- MAX REVISIONS REACHED ---\")\n",
        "        return \"generate_docs\"\n",
        "    else:\n",
        "        return \"continue_refining\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"validator\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue_refining\": \"code_generator_agent\",\n",
        "        \"generate_docs\": \"documentation_generator\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "print(\"--- Graph Compiled Successfully ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdhSLHMy9FyK",
        "outputId": "0716b7df-c169-4be6-aee8-95137f4f9ba0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Constructing Agent Graph ---\n",
            "--- Graph Compiled Successfully ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "Fx_1KfHa92A9",
        "outputId": "308b30db-fee6-4578-972a-c5a9d9dd3580"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x7c57719c70e0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. EXECUTE THE AGENT\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Agent Execution ---\")\n",
        "\n",
        "# Define the problem for the agent\n",
        "inputs = {\n",
        "    \"prompt\": \"Write MicroPython code for a Raspberry Pi Pico to read temperature and humidity from a DHT22 sensor connected to GPIO pin 15. The readings should be printed to the console every 5 seconds.\",\n",
        "    \"data_sources\": [\"https://www.tomshardware.com/how-to/raspberry-pi-pico-dht11-dht22-sensor\"] # Corrected URL format\n",
        "}\n",
        "\n",
        "final_state = None\n",
        "# Stream the output to see the step-by-step process\n",
        "for output in app.stream(inputs, {\"recursion_limit\": 10}):\n",
        "    for key, value in output.items():\n",
        "        print(f\"\\n## Output from node: '{key}' ##\")\n",
        "        # You can print the full value for debugging if needed\n",
        "        # print(value)\n",
        "    final_state = list(output.values())[0] # Capture the latest state\n",
        "\n",
        "print(\"\\n\\n--- Agent Execution Complete ---\")\n",
        "\n",
        "# Print the final results\n",
        "if final_state:\n",
        "    print(\"\\n\\n--- FINAL GENERATED CODE ---\")\n",
        "    print(\"```python\")\n",
        "    print(final_state.get(\"code\", \"Code not generated.\"))\n",
        "    print(\"```\")\n",
        "\n",
        "    print(\"\\n\\n--- FINAL GENERATED DOCUMENTATION ---\")\n",
        "    print(final_state.get(\"documentation\", \"Documentation not generated.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "esPiO3r36-aN",
        "outputId": "2028fdb1-9500-4323-c6be-cf9392736a51"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Agent Execution ---\n",
            "--- RAG SETUP: Loading existing Chroma DB from ./chroma_db ---\n",
            "--- RAG SETUP: Advanced RAG tool with reranking is ready. ---\n",
            "\n",
            "## Output from node: 'rag_setup' ##\n",
            "--- PLANNER AGENT: Starting... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2023168555.py:16: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'HuggingFacePipeline' object has no attribute 'bind_tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1650881699.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Stream the output to see the step-by-step process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"recursion_limit\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n## Output from node: '{key}' ##\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2023168555.py\u001b[0m in \u001b[0;36mplanner_agent_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Assuming llm and state[\"tools\"] are defined globally or in previous cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Bind tools to the LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mllm_with_tools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tools\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tool_calling_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_with_tools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tools\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Configure AgentExecutor with memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    989\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                         \u001b[0;31m# this is the current error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(self).__name__!r} object has no attribute {item!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFacePipeline' object has no attribute 'bind_tools'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BYl1beVEFH9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6Oh8du2FH6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RS891610FH3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gN7FPG8IFHyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEM_eaU5FHvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. Installation (if not already installed)\n",
        "# ==============================================================================\n",
        "# !pip install torch transformers sentence-transformers accelerate bitsandbytes\n",
        "# !pip install chromadb langchain langchain_community langchain_huggingface langgraph\n",
        "# !pip install --upgrade langgraph\n",
        "# !pip install pypdf reportlab # Added for PDF processing and creation\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Imports\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import getpass\n",
        "import re\n",
        "from typing import TypedDict, List, Optional\n",
        "\n",
        "# LangChain and LangGraph components\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Transformers for Hugging Face models\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. DEFINE THE AGENT STATE\n",
        "# ==============================================================================\n",
        "class AgentState(TypedDict):\n",
        "    prompt: str\n",
        "    data_sources: Optional[List[str]]\n",
        "    plan: dict\n",
        "    code: Optional[str]\n",
        "    critique: Optional[str]\n",
        "    documentation: Optional[str]\n",
        "    revision_number: int\n",
        "    messages: List[BaseMessage]\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. SETUP HUGGING FACE MODELS\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Main Language Model (LLM) ---\n",
        "# We are using a more powerful, instruction-tuned model capable of complex reasoning.\n",
        "\n",
        "# Configuration for 4-bit quantization to save memory\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ.get(\"HF_TOKEN\"))\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        "    token=os.environ.get(\"HF_TOKEN\")\n",
        ")\n",
        "\n",
        "# Create a Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=4096, # Increased token limit for code generation\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False, # Important for chat-like interactions\n",
        ")\n",
        "# Wrap the pipeline in the LangChain object\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "\n",
        "# --- Embedding Model for RAG ---\n",
        "print(\"Loading Embedding Model: BAAI/bge-large-en-v1.5\")\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "# --- Reranker Model for RAG Accuracy ---\n",
        "print(\"Loading Reranker Model: BAAI/bge-reranker-large\")\n",
        "reranker_model = HuggingFaceCrossEncoder(\n",
        "    model_name=\"BAAI/bge-reranker-large\",\n",
        "    model_kwargs={'device': device}\n",
        ")\n",
        "print(\"--- All models loaded successfully ---\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. DEFINE TOOLS\n",
        "# ==============================================================================\n",
        "\n",
        "# --- RAG Tool Setup ---\n",
        "def create_rag_tool(data_sources: List[str]):\n",
        "    \"\"\"Dynamically creates a RAG tool from a list of web or PDF sources.\"\"\"\n",
        "    if not data_sources:\n",
        "        print(\"--- RAG: No data sources provided. Skipping tool creation. ---\")\n",
        "        return []\n",
        "\n",
        "    docs = []\n",
        "    for source in data_sources:\n",
        "        if source and source.lower().startswith(\"http\"):\n",
        "            print(f\"--- RAG: Loading web page: {source} ---\")\n",
        "            loader = WebBaseLoader([source])\n",
        "            docs.extend(loader.load())\n",
        "        elif source and source.lower().endswith(\".pdf\"):\n",
        "            print(f\"--- RAG: Loading PDF: {source} ---\")\n",
        "            if os.path.exists(source):\n",
        "                loader = PyPDFLoader(source)\n",
        "                docs.extend(loader.load())\n",
        "            else:\n",
        "                print(f\"--- RAG WARNING: PDF file not found at {source} ---\")\n",
        "        elif source:\n",
        "            print(f\"--- RAG WARNING: Unrecognized source type: {source} ---\")\n",
        "\n",
        "    if not docs:\n",
        "        print(\"--- RAG: No documents were successfully loaded. Skipping tool creation. ---\")\n",
        "        return []\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
        "    base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "    compressor = CrossEncoderReranker(model=reranker_model, top_n=3)\n",
        "    compression_retriever = ContextualCompressionRetriever(\n",
        "        base_compressor=compressor, base_retriever=base_retriever\n",
        "    )\n",
        "\n",
        "    rag_tool = create_retriever_tool(\n",
        "        compression_retriever,\n",
        "        \"documentation_search\",\n",
        "        \"Searches technical documentation for hardware specs, library usage, and code examples.\"\n",
        "    )\n",
        "    print(\"--- RAG: Advanced RAG tool with reranking is ready. ---\")\n",
        "    return [rag_tool]\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. DEFINE THE GRAPH NODES\n",
        "# ==============================================================================\n",
        "\n",
        "# --- NODE 1: Planner ---\n",
        "# This node creates a detailed plan for the code generation by acting as a true agent.\n",
        "def planner_node(state: AgentState):\n",
        "    print(\"--- PLANNER: Creating a plan... ---\")\n",
        "    tools = create_rag_tool(state.get(\"data_sources\", []))\n",
        "\n",
        "    llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "    # Prompt template for the planner agent, instructing it to use tools\n",
        "    planner_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are an expert planner. Your task is to create a detailed, structured plan for writing the code requested by the user.\n",
        "        You MUST use your search tool (`documentation_search`) if you need more information about specific libraries, hardware pins, or APIs. Do not guess or hallucinate details.\n",
        "        After using your tools (or if you determine they are not needed), your final response must be ONLY the valid JSON object containing the plan. Do not add any other text or explanations around the JSON.\n",
        "        The JSON object must follow this schema:\n",
        "        {{\n",
        "            \"platform\": \"e.g., arduino, raspberry_pi\",\n",
        "            \"board\": \"Specific board name (e.g., Arduino Uno, Raspberry Pi Pico)\",\n",
        "            \"language\": \"e.g., C++, MicroPython\",\n",
        "            \"components\": [\"List of hardware components\"],\n",
        "            \"pins\": {{\"component_name\": \"pin_number_or_connection\"}},\n",
        "            \"logic_steps\": [\"Step-by-step logic for the code\"],\n",
        "            \"required_libraries\": [\"List of necessary libraries\"]\n",
        "        }}\"\"\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ])\n",
        "\n",
        "    # Create the tool-calling agent and executor\n",
        "    agent = create_tool_calling_agent(llm_with_tools, tools, planner_prompt)\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    # Invoke the agent to get the plan\n",
        "    result = agent_executor.invoke({\"input\": state[\"prompt\"]})\n",
        "    output_str = result[\"output\"]\n",
        "\n",
        "    # Robustly parse the JSON output from the agent\n",
        "    try:\n",
        "        plan = json.loads(output_str)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"--- PLANNER ERROR: Failed to parse JSON. Attempting to extract from markdown. ---\")\n",
        "        match = re.search(r\"```json\\n(.*)\\n```\", output_str, re.DOTALL)\n",
        "        if match:\n",
        "            plan = json.loads(match.group(1))\n",
        "        else:\n",
        "            raise ValueError(\"Planner agent did not return a valid JSON plan.\")\n",
        "\n",
        "    print(\"--- PLANNER: Plan generated. ---\")\n",
        "    messages = [\n",
        "        HumanMessage(content=state[\"prompt\"]),\n",
        "        AIMessage(content=f\"Understood. Here is the plan to generate the code:\\n\\n```json\\n{json.dumps(plan, indent=2)}\\n```\")\n",
        "    ]\n",
        "    return {\"plan\": plan, \"messages\": messages}\n",
        "\n",
        "\n",
        "# --- NODE 2: Code Generator ---\n",
        "def code_generator_node(state: AgentState):\n",
        "    print(\"--- CODER: Generating code... ---\")\n",
        "    generator_prompt = f\"\"\"You are an expert programmer. Generate the complete, runnable code based on the provided plan.\n",
        "    Only output the raw code inside a single code block (e.g., ```python ... ```). Do not add any other text or explanations.\n",
        "\n",
        "    Plan:\n",
        "    {json.dumps(state[\"plan\"], indent=2)}\n",
        "\n",
        "    {'Critique from previous attempt: ' + state['critique'] if state.get('critique') else ''}\n",
        "    \"\"\"\n",
        "    result = llm.invoke(generator_prompt)\n",
        "\n",
        "    # More robustly extract code from a markdown block\n",
        "    match = re.search(r\"```(?:\\w+\\n)?(.*)```\", result, re.DOTALL)\n",
        "    if match:\n",
        "        code = match.group(1).strip()\n",
        "    else:\n",
        "        code = result.strip() # Fallback if no markdown block is found\n",
        "\n",
        "    print(\"--- CODER: Code generated. ---\")\n",
        "    messages = state[\"messages\"] + [AIMessage(content=f\"Here is the generated code:\\n\\n```{state['plan'].get('language', '')}\\n{code}\\n```\")]\n",
        "    return {\n",
        "        \"code\": code,\n",
        "        \"revision_number\": state.get(\"revision_number\", 0) + 1,\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "# --- NODE 3: Validator / Code Critiquer ---\n",
        "def validator_node(state: AgentState):\n",
        "    print(\"--- VALIDATOR: Reviewing code... ---\")\n",
        "    validator_prompt = f\"\"\"You are a senior code reviewer. Your task is to review the generated code against the original plan.\n",
        "    If the code is correct, complete, and perfectly follows the plan, respond with the single word \"OK\".\n",
        "    Otherwise, provide a concise, bulleted list of errors, missing parts, or suggestions for fixing the code. Be specific and constructive.\n",
        "\n",
        "    Plan: {json.dumps(state['plan'])}\n",
        "    Code:\n",
        "    ```{state['plan'].get('language', '')}\n",
        "    {state['code']}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    critique = llm.invoke(validator_prompt)\n",
        "\n",
        "    if \"OK\" in critique.upper():\n",
        "        print(\"--- VALIDATOR: Code is good! ---\")\n",
        "        messages = state[\"messages\"] + [AIMessage(content=\"The code has been validated and looks correct.\")]\n",
        "        return {\"critique\": None, \"messages\": messages}\n",
        "    else:\n",
        "        print(f\"--- VALIDATOR: Found issues. Critique: {critique} ---\")\n",
        "        messages = state[\"messages\"] + [AIMessage(content=f\"The code has some issues. Here is the critique:\\n\\n{critique}\")]\n",
        "        return {\"critique\": critique, \"messages\": messages}\n",
        "\n",
        "# --- NODE 4: Documentation Generator ---\n",
        "def documentation_generator_node(state: AgentState):\n",
        "    print(\"--- DOCUMENTER: Creating documentation... ---\")\n",
        "    doc_prompt = f\"\"\"You are a technical writer. Generate a comprehensive README.md for this project.\n",
        "    Include the following sections:\n",
        "    1.  **Project Overview**: A brief description of what the project does.\n",
        "    2.  **Hardware Requirements**: A list of all components from the plan.\n",
        "    3.  **Wiring Diagram**: A clear description of how to connect the components.\n",
        "    4.  **Software Setup**: Instructions on how to set up the environment and install libraries.\n",
        "    5.  **Code Explanation**: A step-by-step breakdown of how the code works.\n",
        "\n",
        "    Original Request: {state['prompt']}\n",
        "    Plan: {json.dumps(state['plan'])}\n",
        "    Final Code:\n",
        "    ```{state['plan'].get('language', '')}\n",
        "    {state['code']}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    documentation = llm.invoke(doc_prompt)\n",
        "    print(\"--- DOCUMENTER: Complete. ---\")\n",
        "    messages = state[\"messages\"] + [AIMessage(content=f\"Finally, here is the documentation for the project:\\n\\n{documentation}\")]\n",
        "    return {\"documentation\": documentation, \"messages\": messages}\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. CONSTRUCT THE GRAPH\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"--- Constructing Agent Graph ---\")\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"planner\", planner_node)\n",
        "workflow.add_node(\"code_generator\", code_generator_node)\n",
        "workflow.add_node(\"validator\", validator_node)\n",
        "workflow.add_node(\"documentation_generator\", documentation_generator_node)\n",
        "\n",
        "workflow.set_entry_point(\"planner\")\n",
        "workflow.add_edge(\"planner\", \"code_generator\")\n",
        "workflow.add_edge(\"code_generator\", \"validator\")\n",
        "workflow.add_edge(\"documentation_generator\", END)\n",
        "\n",
        "def validation_router(state: AgentState):\n",
        "    \"\"\"Determines the next step after validation.\"\"\"\n",
        "    if state.get(\"critique\") is None:\n",
        "        print(\"--- ROUTER: Validation successful. Proceeding to documentation. ---\")\n",
        "        return \"generate_docs\"\n",
        "    if state.get(\"revision_number\", 0) > 3:\n",
        "        print(\"--- ROUTER: Max revisions reached. Proceeding to documentation. ---\")\n",
        "        return \"generate_docs\"\n",
        "    else:\n",
        "        print(\"--- ROUTER: Validation failed. Returning to code generator. ---\")\n",
        "        return \"refine_code\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"validator\",\n",
        "    validation_router,\n",
        "    {\"refine_code\": \"code_generator\", \"generate_docs\": \"documentation_generator\"},\n",
        ")\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"--- Graph Compiled Successfully ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. EXECUTE THE AGENT\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Agent Execution ---\")\n",
        "\n",
        "try:\n",
        "    from reportlab.pdfgen import canvas\n",
        "    from reportlab.lib.pagesizes import letter\n",
        "    pdf_path = \"dht22_pico_guide.pdf\"\n",
        "    c = canvas.Canvas(pdf_path, pagesize=letter)\n",
        "    c.drawString(72, 800, \"Raspberry Pi Pico and DHT22 Guide\")\n",
        "    c.drawString(72, 780, \"The DHT22 sensor is a popular choice for measuring temperature and humidity.\")\n",
        "    c.drawString(72, 760, \"To use it with a Raspberry Pi Pico, you need the 'dht' library.\")\n",
        "    c.drawString(72, 740, \"Connect the VCC pin to 3.3V, GND to GND, and the Data pin to a GPIO pin, like GP15.\")\n",
        "    c.save()\n",
        "    print(f\"--- Created dummy PDF for testing: {pdf_path} ---\")\n",
        "except ImportError:\n",
        "    print(\"--- reportlab not found, skipping dummy PDF creation. `pip install reportlab` to enable. ---\")\n",
        "    pdf_path = None\n",
        "\n",
        "inputs = {\n",
        "    \"prompt\": \"Write MicroPython code for a Raspberry Pi Pico to read temperature and humidity from a DHT22 sensor connected to GPIO pin 15. The readings should be printed to the console every 5 seconds.\",\n",
        "    \"data_sources\": [\n",
        "        \"https://www.tomshardware.com/how-to/raspberry-pi-pico-dht11-dht22-sensor\",\n",
        "        pdf_path\n",
        "    ]\n",
        "}\n",
        "\n",
        "final_state = {}\n",
        "for output in app.stream(inputs, {\"recursion_limit\": 15}):\n",
        "    for key, value in output.items():\n",
        "        print(f\"\\n## Output from node: '{key}' ##\")\n",
        "        final_state.update(value)\n",
        "\n",
        "print(\"\\n\\n--- Agent Execution Complete ---\")\n",
        "\n",
        "if final_state:\n",
        "    print(\"\\n\\n--- FINAL GENERATED CODE ---\")\n",
        "    print(f\"```{final_state.get('plan', {}).get('language', '')}\")\n",
        "    print(final_state.get(\"code\", \"Code not generated.\"))\n",
        "    print(\"```\")\n",
        "\n",
        "    print(\"\\n\\n--- FINAL GENERATED DOCUMENTATION ---\")\n",
        "    print(final_state.get(\"documentation\", \"Documentation not generated.\"))\n",
        "\n",
        "    print(\"\\n\\n--- FULL CONVERSATION LOG ---\")\n",
        "    for msg in final_state.get(\"messages\", []):\n",
        "        print(f\"[{msg.type.upper()}]:\")\n",
        "        content = msg.content\n",
        "        # Pretty print JSON content if it's a string that looks like JSON\n",
        "        if isinstance(content, str) and content.strip().startswith('{'):\n",
        "            try:\n",
        "                parsed_json = json.loads(content)\n",
        "                print(json.dumps(parsed_json, indent=2))\n",
        "            except json.JSONDecodeError:\n",
        "                print(content)\n",
        "        else:\n",
        "            print(content)\n",
        "        print(\"-\" * 20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "Rv4NiUy7FHs4",
        "outputId": "ca01f5a4-70ad-4a7e-c1f3-b5b2d694fa5d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Embedding Model: BAAI/bge-large-en-v1.5\n",
            "Loading Reranker Model: BAAI/bge-reranker-large\n",
            "--- All models loaded successfully ---\n",
            "--- Constructing Agent Graph ---\n",
            "--- Graph Compiled Successfully ---\n",
            "\n",
            "--- Starting Agent Execution ---\n",
            "--- reportlab not found, skipping dummy PDF creation. `pip install reportlab` to enable. ---\n",
            "--- PLANNER: Creating a plan... ---\n",
            "--- RAG: Loading web page: https://www.tomshardware.com/how-to/raspberry-pi-pico-dht11-dht22-sensor ---\n",
            "--- RAG: Advanced RAG tool with reranking is ready. ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'HuggingFacePipeline' object has no attribute 'bind_tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2343890263.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"recursion_limit\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n## Output from node: '{key}' ##\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2343890263.py\u001b[0m in \u001b[0;36mplanner_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mtools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_rag_tool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_sources\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mllm_with_tools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# Prompt template for the planner agent, instructing it to use tools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    989\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                         \u001b[0;31m# this is the current error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(self).__name__!r} object has no attribute {item!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFacePipeline' object has no attribute 'bind_tools'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PR6nxWtgG7KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uxrJUv9GG7GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-tavily\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCOZb4dcHUIa",
        "outputId": "f2ebca11-0be3-4ad9-cc84-d95c4fb64b8e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-tavily in /usr/local/lib/python3.12/dist-packages (0.2.11)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.12/dist-packages (from langchain-tavily) (3.12.15)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.20 in /usr/local/lib/python3.12/dist-packages (from langchain-tavily) (0.3.27)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.12/dist-packages (from langchain-tavily) (0.3.74)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from langchain-tavily) (2.32.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.20.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2025.8.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-tavily) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.20->langchain-tavily) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_tavily import TavilySearch"
      ],
      "metadata": {
        "id": "Kfsg7k0BH2jC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. Installation (if not already installed)\n",
        "# ==============================================================================\n",
        "# !pip install torch transformers sentence-transformers accelerate bitsandbytes\n",
        "# !pip install chromadb langchain langchain_community langchain_huggingface langgraph\n",
        "# !pip install --upgrade langgraph\n",
        "# !pip install tavily # Added for Tavily search tool\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Imports\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import getpass\n",
        "import re\n",
        "from typing import TypedDict, List, Optional\n",
        "\n",
        "# LangChain and LangGraph components\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Transformers for Hugging Face models\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Securely Set API Keys\n",
        "# ==============================================================================\n",
        "# NOTE: HF_TOKEN is not required for distilgpt2, but good practice for gated models.\n",
        "# if \"HF_TOKEN\" not in os.environ:\n",
        "#     try:\n",
        "#         os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Could not read Hugging Face token: {e}\")\n",
        "\n",
        "# Securely set Tavily API key, which is required for the search tool\n",
        "if \"TAVILY_API_KEY\" not in os.environ:\n",
        "    try:\n",
        "        os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not read Tavily API key: {e}\")\n",
        "        # The agent will fail without this key.\n",
        "        exit()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. DEFINE THE AGENT STATE\n",
        "# ==============================================================================\n",
        "class AgentState(TypedDict):\n",
        "    prompt: str\n",
        "    plan: dict\n",
        "    code: Optional[str]\n",
        "    critique: Optional[str]\n",
        "    documentation: Optional[str]\n",
        "    revision_number: int\n",
        "    messages: List[BaseMessage]\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. SETUP HUGGING FACE MODELS\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Main Language Model (LLM) ---\n",
        "# CRITICAL NOTE: The model has been changed to 'distilbert/distilgpt2' as requested.\n",
        "# This model is a very small, non-instruction-tuned model. It does NOT have the\n",
        "# capability to perform the complex tasks required by this agent, such as:\n",
        "#   - Following instructions to generate structured JSON.\n",
        "#   - Reasoning about when to use tools like Tavily Search.\n",
        "#   - Generating coherent code from a plan.\n",
        "# As a result, while the code is now syntactically correct, THE AGENT WILL NOT\n",
        "# FUNCTION AS INTENDED. For this agent to work, a powerful, instruction-tuned\n",
        "# model like 'meta-llama/Meta-Llama-3-8B-Instruct' is required.\n",
        "model_id = \"distilbert/distilgpt2\"\n",
        "print(f\"Loading Main LLM: {model_id}\")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Create a Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=1024, # Reduced for smaller model\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False, # Important for chat-like interactions\n",
        ")\n",
        "# Wrap the pipeline in the LangChain object\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "print(\"--- LLM loaded successfully ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX6QKZ7_IH58",
        "outputId": "ea051e73-4906-4358-c9de-22ab319eee99"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading Main LLM: distilbert/distilgpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- LLM loaded successfully ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. DEFINE THE GRAPH NODES\n",
        "# ==============================================================================\n",
        "\n",
        "# --- NODE 1: Planner ---\n",
        "# This node creates a detailed plan for the code generation by acting as a true agent.\n",
        "def planner_node(state: AgentState):\n",
        "    print(\"--- PLANNER: Creating a plan... ---\")\n",
        "    # Initialize the Tavily search tool\n",
        "    tools = [TavilySearchResults(max_results=3)]\n",
        "\n",
        "    # Prompt template for the planner agent, instructing it to use the search tool\n",
        "    planner_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are an expert planner. Your task is to create a detailed, structured plan for writing the code requested by the user.\n",
        "        You MUST use your search tool to find information about specific libraries, hardware pins, or APIs if you are unsure. Do not guess or hallucinate details.\n",
        "        After using your tools (or if you determine they are not needed), your final response must be ONLY the valid JSON object containing the plan. Do not add any other text or explanations around the JSON.\n",
        "        The JSON object must follow this schema:\n",
        "        {{\n",
        "            \"platform\": \"e.g., arduino, raspberry_pi\",\n",
        "            \"board\": \"Specific board name (e.g., Arduino Uno, Raspberry Pi Pico)\",\n",
        "            \"language\": \"e.g., C++, MicroPython\",\n",
        "            \"components\": [\"List of hardware components\"],\n",
        "            \"pins\": {{\"component_name\": \"pin_number_or_connection\"}},\n",
        "            \"logic_steps\": [\"Step-by-step logic for the code\"],\n",
        "            \"required_libraries\": [\"List of necessary libraries\"]\n",
        "        }}\"\"\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ])\n",
        "\n",
        "    # Create the tool-calling agent and executor\n",
        "    agent = create_tool_calling_agent(llm, tools, planner_prompt)\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    # Invoke the agent to get the plan\n",
        "    result = agent_executor.invoke({\"input\": state[\"prompt\"]})\n",
        "    output_str = result[\"output\"]\n",
        "\n",
        "    # Robustly parse the JSON output from the agent\n",
        "    try:\n",
        "        plan = json.loads(output_str)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"--- PLANNER ERROR: Failed to parse JSON. Attempting to extract from markdown. ---\")\n",
        "        match = re.search(r\"```json\\n(.*)\\n```\", output_str, re.DOTALL)\n",
        "        if match:\n",
        "            plan = json.loads(match.group(1))\n",
        "        else:\n",
        "            # This error is expected with distilgpt2 as it cannot reliably produce JSON.\n",
        "            raise ValueError(\"Planner agent did not return a valid JSON plan.\")\n",
        "\n",
        "    print(\"--- PLANNER: Plan generated. ---\")\n",
        "    messages = [\n",
        "        HumanMessage(content=state[\"prompt\"]),\n",
        "        AIMessage(content=f\"Understood. Here is the plan to generate the code:\\n\\n```json\\n{json.dumps(plan, indent=2)}\\n```\")\n",
        "    ]\n",
        "    return {\"plan\": plan, \"messages\": messages}\n",
        "\n",
        "\n",
        "# --- NODE 2: Code Generator ---\n",
        "def code_generator_node(state: AgentState):\n",
        "    print(\"--- CODER: Generating code... ---\")\n",
        "    generator_prompt = f\"\"\"You are an expert programmer. Generate the complete, runnable code based on the provided plan.\n",
        "    Only output the raw code inside a single code block (e.g., ```python ... ```). Do not add any other text or explanations.\n",
        "\n",
        "    Plan:\n",
        "    {json.dumps(state[\"plan\"], indent=2)}\n",
        "\n",
        "    {'Critique from previous attempt: ' + state['critique'] if state.get('critique') else ''}\n",
        "    \"\"\"\n",
        "    result = llm.invoke(generator_prompt)\n",
        "\n",
        "    # More robustly extract code from a markdown block\n",
        "    match = re.search(r\"```(?:\\w+\\n)?(.*)```\", result, re.DOTALL)\n",
        "    if match:\n",
        "        code = match.group(1).strip()\n",
        "    else:\n",
        "        code = result.strip() # Fallback if no markdown block is found\n",
        "\n",
        "    print(\"--- CODER: Code generated. ---\")\n",
        "    messages = state[\"messages\"] + [AIMessage(content=f\"Here is the generated code:\\n\\n```{state['plan'].get('language', '')}\\n{code}\\n```\")]\n",
        "    return {\n",
        "        \"code\": code,\n",
        "        \"revision_number\": state.get(\"revision_number\", 0) + 1,\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "# --- NODE 3: Validator / Code Critiquer ---\n",
        "def validator_node(state: AgentState):\n",
        "    print(\"--- VALIDATOR: Reviewing code... ---\")\n",
        "    validator_prompt = f\"\"\"You are a senior code reviewer. Your task is to review the generated code against the original plan.\n",
        "    If the code is correct, complete, and perfectly follows the plan, respond with the single word \"OK\".\n",
        "    Otherwise, provide a concise, bulleted list of errors, missing parts, or suggestions for fixing the code. Be specific and constructive.\n",
        "\n",
        "    Plan: {json.dumps(state['plan'])}\n",
        "    Code:\n",
        "    ```{state['plan'].get('language', '')}\n",
        "    {state['code']}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    critique = llm.invoke(validator_prompt)\n",
        "\n",
        "    if \"OK\" in critique.upper():\n",
        "        print(\"--- VALIDATOR: Code is good! ---\")\n",
        "        messages = state[\"messages\"] + [AIMessage(content=\"The code has been validated and looks correct.\")]\n",
        "        return {\"critique\": None, \"messages\": messages}\n",
        "    else:\n",
        "        print(f\"--- VALIDATOR: Found issues. Critique: {critique} ---\")\n",
        "        messages = state[\"messages\"] + [AIMessage(content=f\"The code has some issues. Here is the critique:\\n\\n{critique}\")]\n",
        "        return {\"critique\": critique, \"messages\": messages}\n",
        "\n",
        "# --- NODE 4: Documentation Generator ---\n",
        "def documentation_generator_node(state: AgentState):\n",
        "    print(\"--- DOCUMENTER: Creating documentation... ---\")\n",
        "    doc_prompt = f\"\"\"You are a technical writer. Generate a comprehensive README.md for this project.\n",
        "    Include the following sections:\n",
        "    1.  **Project Overview**: A brief description of what the project does.\n",
        "    2.  **Hardware Requirements**: A list of all components from the plan.\n",
        "    3.  **Wiring Diagram**: A clear description of how to connect the components.\n",
        "    4.  **Software Setup**: Instructions on how to set up the environment and install libraries.\n",
        "    5.  **Code Explanation**: A step-by-step breakdown of how the code works.\n",
        "\n",
        "    Original Request: {state['prompt']}\n",
        "    Plan: {json.dumps(state['plan'])}\n",
        "    Final Code:\n",
        "    ```{state['plan'].get('language', '')}\n",
        "    {state['code']}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    documentation = llm.invoke(doc_prompt)\n",
        "    print(\"--- DOCUMENTER: Complete. ---\")\n",
        "    messages = state[\"messages\"] + [AIMessage(content=f\"Finally, here is the documentation for the project:\\n\\n{documentation}\")]\n",
        "    return {\"documentation\": documentation, \"messages\": messages}\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. CONSTRUCT THE GRAPH\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"--- Constructing Agent Graph ---\")\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"planner\", planner_node)\n",
        "workflow.add_node(\"code_generator\", code_generator_node)\n",
        "workflow.add_node(\"validator\", validator_node)\n",
        "workflow.add_node(\"documentation_generator\", documentation_generator_node)\n",
        "\n",
        "workflow.set_entry_point(\"planner\")\n",
        "workflow.add_edge(\"planner\", \"code_generator\")\n",
        "workflow.add_edge(\"code_generator\", \"validator\")\n",
        "workflow.add_edge(\"documentation_generator\", END)\n",
        "\n",
        "def validation_router(state: AgentState):\n",
        "    \"\"\"Determines the next step after validation.\"\"\"\n",
        "    if state.get(\"critique\") is None:\n",
        "        print(\"--- ROUTER: Validation successful. Proceeding to documentation. ---\")\n",
        "        return \"generate_docs\"\n",
        "    if state.get(\"revision_number\", 0) > 3:\n",
        "        print(\"--- ROUTER: Max revisions reached. Proceeding to documentation. ---\")\n",
        "        return \"generate_docs\"\n",
        "    else:\n",
        "        print(\"--- ROUTER: Validation failed. Returning to code generator. ---\")\n",
        "        return \"refine_code\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"validator\",\n",
        "    validation_router,\n",
        "    {\"refine_code\": \"code_generator\", \"generate_docs\": \"documentation_generator\"},\n",
        ")\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"--- Graph Compiled Successfully ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. EXECUTE THE AGENT\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Agent Execution ---\")\n",
        "\n",
        "# The 'data_sources' key is no longer needed as we are using Tavily for general search.\n",
        "inputs = {\n",
        "    \"prompt\": \"Write MicroPython code for a Raspberry Pi Pico to read temperature and humidity from a DHT22 sensor connected to GPIO pin 15. The readings should be printed to the console every 5 seconds.\",\n",
        "}\n",
        "\n",
        "final_state = {}\n",
        "try:\n",
        "    for output in app.stream(inputs, {\"recursion_limit\": 15}):\n",
        "        for key, value in output.items():\n",
        "            print(f\"\\n## Output from node: '{key}' ##\")\n",
        "            final_state.update(value)\n",
        "except ValueError as e:\n",
        "    print(f\"\\n--- AGENT FAILED ---\")\n",
        "    print(f\"An error occurred, likely due to the limitations of the 'distilbert/distilgpt2' model.\")\n",
        "    print(f\"Error details: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n--- Agent Execution Complete ---\")\n",
        "\n",
        "if final_state and \"code\" in final_state:\n",
        "    print(\"\\n\\n--- FINAL GENERATED CODE ---\")\n",
        "    print(f\"```{final_state.get('plan', {}).get('language', '')}\")\n",
        "    print(final_state.get(\"code\", \"Code not generated.\"))\n",
        "    print(\"```\")\n",
        "\n",
        "    print(\"\\n\\n--- FINAL GENERATED DOCUMENTATION ---\")\n",
        "    print(final_state.get(\"documentation\", \"Documentation not generated.\"))\n",
        "\n",
        "    print(\"\\n\\n--- FULL CONVERSATION LOG ---\")\n",
        "    for msg in final_state.get(\"messages\", []):\n",
        "        print(f\"[{msg.type.upper()}]:\")\n",
        "        content = msg.content\n",
        "        # Pretty print JSON content if it's a string that looks like JSON\n",
        "        if isinstance(content, str) and content.strip().startswith('{'):\n",
        "            try:\n",
        "                parsed_json = json.loads(content)\n",
        "                print(json.dumps(parsed_json, indent=2))\n",
        "            except json.JSONDecodeError:\n",
        "                print(content)\n",
        "        else:\n",
        "            print(content)\n",
        "        print(\"-\" * 20)\n",
        "else:\n",
        "    print(\"\\n--- No final output was generated. The agent likely failed in an early step. ---\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pr20OXQG7Di",
        "outputId": "2b3ca2e6-14f2-4a43-8ea9-67c7f8e63eb6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Constructing Agent Graph ---\n",
            "--- Graph Compiled Successfully ---\n",
            "\n",
            "--- Starting Agent Execution ---\n",
            "--- PLANNER: Creating a plan... ---\n",
            "\n",
            "--- AGENT FAILED ---\n",
            "An error occurred, likely due to the limitations of the 'distilbert/distilgpt2' model.\n",
            "Error details: This function requires a bind_tools() method be implemented on the LLM.\n",
            "\n",
            "\n",
            "--- Agent Execution Complete ---\n",
            "\n",
            "--- No final output was generated. The agent likely failed in an early step. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n792rlyNHglJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}