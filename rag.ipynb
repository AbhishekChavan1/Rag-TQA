{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aS_JGU-z8jWr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "410303f5-bdae-4826-f179-1b8745cffc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.20 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "opentelemetry"
                ]
              },
              "id": "ab24550e7d974eff9495105afedfe814"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install core libraries\n",
        "!pip install langchain langchain_community langchain_core langchain-huggingface\n",
        "!pip install transformers torch accelerate bitsandbytes\n",
        "!pip install sentence-transformers faiss-cpu pypdf\n",
        "\n",
        "# Install advanced components\n",
        "!pip install rank_bm25\n",
        "!pip install langchain-cohere  # For reranker, requires a free trial API key\n",
        "!pip install langgraph\n",
        "\n",
        "# Install evaluation libraries\n",
        "!pip install evaluate rouge_score nltk\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4XXT9-D68jRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4426a19e-79cd-435e-be35-b469aa333d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.schema.document import Document\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_cohere.rerank import CohereRerank\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, List\n",
        "from sklearn.metrics import dcg_score, ndcg_score\n",
        "import os\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "LDeSB-z--ffV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7gh5jHam8jO5"
      },
      "outputs": [],
      "source": [
        "# TEMPLATE Configuration\n",
        "CONFIG = {\n",
        "    \"llm\": {\n",
        "        \"model_id\": \"HuggingFaceH4/zephyr-7b-beta\", # A great, lightweight model for Colab\n",
        "        # \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.2\", # Another excellent option\n",
        "    },\n",
        "    \"embedding\": {\n",
        "        \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    },\n",
        "    \"retriever\": {\n",
        "        \"vector_store\": \"faiss\",\n",
        "        \"k\": 5, # Number of documents to retrieve\n",
        "    },\n",
        "    \"reranker\": {\n",
        "        # Using Cohere's free-for-dev reranker for high quality.\n",
        "        # Alternatively, a local cross-encoder could be used.\n",
        "        \"model_id\": \"cohere-rerank-english-v3.0\",\n",
        "        \"top_n\": 2, # Number of documents to keep after reranking\n",
        "    },\n",
        "    \"text_splitter\": {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 100,\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"source\": \"web\", # Can be 'pdf', 'web', 'txt'\n",
        "        \"path\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\", # Example blog post\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "px3_dKVg8jMF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "9f64190f7ea64a47a88c464f6f423127",
            "fb360d7f740643de9decc50ff1e76934",
            "eed99ee3b4f94c12ac93f351a68b6157",
            "719e8d9a15424409b8f48bbffd076594",
            "46b6be703ec148b28af951cde405eb9d",
            "a38cb92c7c5b49cc9fe6d9ba269f02e0",
            "c0425d8f98a646e9ba60472e8650ee54",
            "91148186172d4ccdafc6f41630551829",
            "9bd9cd048ec048d78c497a6ac5adcc5c",
            "5c768fad76414fef968eb13db0508f06",
            "9b1ea47fca284bf4a3bb76551276b604"
          ]
        },
        "outputId": "dfa99323-bdf8-409e-c23b-ad1611c6ed18"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f64190f7ea64a47a88c464f6f423127",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Initialized Successfully.\n"
          ]
        }
      ],
      "source": [
        "def get_llm(config):\n",
        "    \"\"\"Initializes and returns the LLM for inference.\"\"\"\n",
        "    # Configuration for 4-bit quantization for memory efficiency\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config[\"llm\"][\"model_id\"])\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        config[\"llm\"][\"model_id\"],\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\", # Automatically map to GPU if available\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    # Create a Hugging Face pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.15\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    return llm\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = get_llm(CONFIG)\n",
        "print(\"LLM Initialized Successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_documents(config):\n",
        "    \"\"\"Loads documents from the specified source and splits them into chunks.\"\"\"\n",
        "    path = config[\"data\"][\"path\"]\n",
        "    if config[\"data\"][\"source\"] == \"web\":\n",
        "        loader = WebBaseLoader(path)\n",
        "    elif config[\"data\"][\"source\"] == \"pdf\":\n",
        "        loader = PyPDFLoader(path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported data source\")\n",
        "\n",
        "    documents = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config[\"text_splitter\"][\"chunk_size\"],\n",
        "        chunk_overlap=config[\"text_splitter\"][\"chunk_overlap\"]\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n",
        "\n",
        "# Load and split documents\n",
        "documents_chunks = load_and_split_documents(CONFIG)\n",
        "print(f\"Loaded and split {len(documents_chunks)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e23K7ke53Ksy",
        "outputId": "be25bed6-5395-4844-eb90-6502d3e93f42"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded and split 64 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(chunks, config):\n",
        "    \"\"\"Creates a Chroma vector store from document chunks.\"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=config[\"embedding\"][\"model_id\"])\n",
        "    # Updated to use Chroma\n",
        "    vector_store = Chroma.from_documents(chunks, embeddings)\n",
        "    return vector_store\n",
        "\n",
        "# Create the vector store\n",
        "vector_store = create_vector_store(documents_chunks, CONFIG)\n",
        "print(\"Vector Store created successfully with Chroma DB.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPHANKsc3Ko8",
        "outputId": "acee1674-1d08-413e-acaf-91cc3a96e1df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector Store created successfully with Chroma DB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You need a Cohere API key for this. Get a free trial key from dashboard.cohere.com\n",
        "os.environ[\"COHERE_API_KEY\"] = \"LLPo5KmMNg5BwzC7Xpouw3NekKUW9kFl5uULqxjn\" # Replace with your key\n",
        "\n",
        "def create_retriever(vector_store, config):\n",
        "    \"\"\"Creates a standard retriever and optionally a reranking retriever.\"\"\"\n",
        "    # 1. Base Retriever\n",
        "    base_retriever = vector_store.as_retriever(search_kwargs={\"k\": config[\"retriever\"][\"k\"]})\n",
        "\n",
        "    # 2. Reranker (Optional - uncomment if you have access to a working reranker model)\n",
        "    # try:\n",
        "    #     reranker = CohereRerank(model=config[\"reranker\"][\"model_id\"], top_n=config[\"reranker\"][\"top_n\"])\n",
        "    #     # 3. Contextual Compression Retriever\n",
        "    #     compression_retriever = ContextualCompressionRetriever(\n",
        "    #         base_compressor=reranker, base_retriever=base_retriever\n",
        "    #     )\n",
        "    #     print(\"Retriever with Reranker created.\")\n",
        "    #     return compression_retriever\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Could not create reranker: {e}\")\n",
        "    #     print(\"Using base retriever without reranking.\")\n",
        "    #     return base_retriever\n",
        "\n",
        "    # Using base retriever without reranking due to reranker error\n",
        "    print(\"Using base retriever without reranking.\")\n",
        "    return base_retriever\n",
        "\n",
        "\n",
        "# Create the advanced retriever\n",
        "retriever = create_retriever(vector_store, CONFIG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc7Canxb3Kli",
        "outputId": "d4b09d94-6ab2-4129-f6ad-11e9000d6a2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using base retriever without reranking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the desired structured output\n",
        "class Answer(BaseModel):\n",
        "    answer: str = Field(description=\"The final, detailed answer to the user's question.\")\n",
        "    sources: List[str] = Field(description=\"List of source document snippets used to generate the answer.\")\n",
        "    confidence_score: float = Field(description=\"A score from 0.0 to 1.0 indicating the model's confidence in its answer.\")\n",
        "\n",
        "# Create the JSON output parser\n",
        "output_parser = JsonOutputParser(pydantic_object=Answer)\n",
        "\n",
        "# Define the RAG prompt template\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "<|system|>\n",
        "You are an expert Question-Answering assistant. Your goal is to provide accurate, fact-grounded answers based exclusively on the provided context.\n",
        "Do not add any information that is not present in the context.\n",
        "If the context does not contain the answer, state that you cannot answer the question with the given information.\n",
        "Format your response as a JSON object with the following keys: \"answer\", \"sources\", \"confidence_score\".\n",
        "\"sources\" should be a list of the exact text snippets from the context that justify your answer.\n",
        "</|system|>\n",
        "<|user|>\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "Return a JSON object with the answer.\n",
        "</|user|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = PromptTemplate(\n",
        "    template=RAG_PROMPT_TEMPLATE,\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
        ")"
      ],
      "metadata": {
        "id": "6PeeIhlH4pfL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs: List[Document]) -> str:\n",
        "    \"\"\"Formats the retrieved documents into a single string.\"\"\"\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Create the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | output_parser # Re-enable output_parser\n",
        ")\n",
        "\n",
        "# Test the chain\n",
        "question = \"What are the core components of an LLM-powered autonomous agent system?\"\n",
        "response = rag_chain.invoke(question)\n",
        "\n",
        "print(\"--- RAG Response ---\")\n",
        "print(f\"Answer: {response['answer']}\")\n",
        "print(f\"Confidence: {response['confidence_score']}\")\n",
        "print(\"Sources:\")\n",
        "for source in response['sources']:\n",
        "    print(f\"- {source[:100]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y6QLEZ64pb4",
        "outputId": "f9544ace-9a97-4d37-e6cf-96071628226b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RAG Response (Raw LLM Output) ---\n",
            "\n",
            "<|system|>\n",
            "You are an expert Question-Answering assistant. Your goal is to provide accurate, fact-grounded answers based exclusively on the provided context.\n",
            "Do not add any information that is not present in the context.\n",
            "If the context does not contain the answer, state that you cannot answer the question with the given information.\n",
            "Format your response as a JSON object with the following keys: \"answer\", \"sources\", \"confidence_score\".\n",
            "\"sources\" should be a list of the exact text snippets from the context that justify your answer.\n",
            "</|system|>\n",
            "<|user|>\n",
            "CONTEXT:\n",
            "LLM Powered Autonomous Agents | Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "|\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Archive\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Search\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tags\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FAQ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\n",
            "\n",
            "Agent System Overview\n",
            "\n",
            "Component One: Planning\n",
            "\n",
            "Task Decomposition\n",
            "\n",
            "Self-Reflection\n",
            "\n",
            "\n",
            "Component Two: Memory\n",
            "\n",
            "Types of Memory\n",
            "\n",
            "Maximum Inner Product Search (MIPS)\n",
            "\n",
            "\n",
            "Component Three: Tool Use\n",
            "\n",
            "Case Studies\n",
            "\n",
            "Scientific Discovery Agent\n",
            "\n",
            "Generative Agents Simulation\n",
            "\n",
            "Proof-of-Concept Examples\n",
            "\n",
            "\n",
            "Challenges\n",
            "\n",
            "Citation\n",
            "\n",
            "References\n",
            "\n",
            "Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
            "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
            "\n",
            "\n",
            "Tool use\n",
            "\n",
            "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Overview of a LLM-powered autonomous agent system.\n",
            "\n",
            "Generative Agents Simulation\n",
            "\n",
            "Proof-of-Concept Examples\n",
            "\n",
            "\n",
            "Challenges\n",
            "\n",
            "Citation\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
            "\n",
            "Planning\n",
            "\n",
            "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
            "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
            "\n",
            "\n",
            "Memory\n",
            "\n",
            "}\n",
            "]\n",
            "Challenges#\n",
            "After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\n",
            "\n",
            "Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n",
            "\n",
            "\n",
            "Citation#\n",
            "Cited as:\n",
            "\n",
            "Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n",
            "\n",
            "QUESTION:\n",
            "What are the core components of an LLM-powered autonomous agent system?\n",
            "\n",
            "Return a JSON object with the answer.\n",
            "</|user|>\n",
            "<|assistant|>\n",
            "{\n",
            "  \"answer\": [\n",
            "    \"The core components of an LLM-powered autonomous agent system are Planning, Memory, and Tool use.\",\n",
            "    \"Planning involves breaking down complex tasks into smaller subgoals and allowing the agent to learn from mistakes and improve through self-reflection.\",\n",
            "    \"Memory allows the agent to retain and recall information over long periods using both short-term and long-term memory, either through internal or external storage methods.\",\n",
            "    \"Tool use allows the agent to utilize external APIs to obtain additional information when necessary.\"\n",
            "  ],\n",
            "  \"sources\": [\n",
            "    \"Weng, Lilian. (Jun 2023). ‘LLM-powered Autonomous Agents’. Lil’Log. Retrieved June 24, 2023, from https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
            "  ],\n",
            "  \"confidence_score\": 1\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the state for our graph\n",
        "class GraphState(TypedDict):\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[Document]\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"Retrieve documents.\"\"\"\n",
        "    print(\"---NODE: RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"Generate answer.\"\"\"\n",
        "    print(\"---NODE: GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = rag_chain.invoke(question) # Using our previously defined chain\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"Grade documents for relevance.\"\"\"\n",
        "    print(\"---NODE: GRADE DOCUMENTS---\")\n",
        "    # This would typically involve an LLM call to check if documents are relevant.\n",
        "    # For simplicity, we'll assume they are relevant for now.\n",
        "    return \"generate\" # Decision to proceed to generation\n",
        "\n",
        "# Define the workflow\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "\n",
        "# Define the edges\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\",\n",
        "    grade_documents,\n",
        "    {\"generate\": \"generate\", \"end\": END} # Simplified: always go to generate\n",
        ")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile and run the graph\n",
        "app = workflow.compile()\n",
        "inputs = {\"question\": \"What is the 'memory' component in an agent?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFeUb0UR4pYI",
        "outputId": "594ee61b-083a-407d-940d-a9b17007d4ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---NODE: RETRIEVE---\n",
            "---NODE: GRADE DOCUMENTS---\n",
            "Output from node 'retrieve':\n",
            "---\n",
            "{'documents': [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'), Document(metadata={'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\"), Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en'}, page_content='Categorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.'), Document(metadata={'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Generative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')], 'question': \"What is the 'memory' component in an agent?\"}\n",
            "\n",
            "---\n",
            "\n",
            "---NODE: GENERATE---\n",
            "Output from node 'generate':\n",
            "---\n",
            "{'documents': [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'), Document(metadata={'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\"), Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en'}, page_content='Categorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.'), Document(metadata={'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Generative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')], 'question': \"What is the 'memory' component in an agent?\", 'generation': '\\n<|system|>\\nYou are an expert Question-Answering assistant. Your goal is to provide accurate, fact-grounded answers based exclusively on the provided context.\\nDo not add any information that is not present in the context.\\nIf the context does not contain the answer, state that you cannot answer the question with the given information.\\nFormat your response as a JSON object with the following keys: \"answer\", \"sources\", \"confidence_score\".\\n\"sources\" should be a list of the exact text snippets from the context that justify your answer.\\n</|system|>\\n<|user|>\\nCONTEXT:\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nLLM Powered Autonomous Agents | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nQUESTION:\\nWhat is the \\'memory\\' component in an agent?\\n\\nReturn a JSON object with the answer.\\n</|user|>\\n<|assistant|>\\n{\\n  \"answer\": \"In an LLM-powered autonomous agent system, the\\'memory\\' component refers to the ability of the agent to remember information over extended periods through the utilization of both short-term and long-term memory modules. Short-term memory involves learning within the context window length of transformers, while long-term memory relies on an external vector store that can be accessed via fast retrieval. These memory systems enable the agent to recall relevant information during decision making and improve the efficiency and effectiveness of task completion.\"\\n  \"sources\": [\\n    \"Components of an LLM-powered autonomous agent system.\",\\n    \"Types of memory and their relationship to learning processes in humans and machines.\",\\n    \"Explanation of Maximum Inner Product Search (MIPS) used for efficient retrieval from large databases.\"\\n  ],\\n  \"confidence_score\": 95\\n}'}\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually crafted evaluation dataset\n",
        "# Ground truth document content must match text in the original source\n",
        "EVAL_DATASET = [\n",
        "    {\n",
        "        \"question\": \"What is the 'reflection' component in an LLM-powered agent?\",\n",
        "        \"ground_truth_answer\": \"The 'reflection' component in an LLM-powered agent involves self-criticism and self-correction. It allows the agent to internally reflect on its past actions and outcomes, learn from its mistakes, and refine its plan for future steps. This mechanism is crucial for long-term reasoning and continuous improvement.\",\n",
        "        \"ground_truth_docs_text\": [\n",
        "            \"The LLM acts as the agent’s reasoning engine, which can be further augmented by: a) a planning module for task decomposition and planning, b) a memory stream for storing and retrieving past experiences, and c) a reflection mechanism for self-criticism and self-improvement.\",\n",
        "            \"Reflection is a meta-level process that allows an agent to learn from its past experience and refine its plan for future actions. Reflection is a form of self-criticism and self-correction. It helps agents to not only avoid repeating past mistakes but also to improve their reasoning and planning skills over time.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Explain the key components of a generic agent system.\",\n",
        "        \"ground_truth_answer\": \"A generic agent system consists of three main components: Planning (for task decomposition), Memory (for storing past experiences), and Tool Use (for interacting with the external world to perform tasks).\",\n",
        "        \"ground_truth_docs_text\": [\n",
        "            \"The agent uses the LLM as its core reasoning engine. The three key components of the generic agent system are: Planning (for task decomposition and strategic thinking), Memory (for storing and retrieving information), and Tool Use (for interacting with external environments).\"\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "G045Misn5a7U"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_retrieval_metrics(retriever, documents_chunks, eval_dataset):\n",
        "    print(\"--- Calculating Retrieval Metrics ---\")\n",
        "    retrieval_results = []\n",
        "    k = CONFIG[\"retriever\"][\"k\"]\n",
        "\n",
        "    # Map ground truth text to document indices for NDCG calculation\n",
        "    text_to_doc_idx = {doc.page_content: i for i, doc in enumerate(documents_chunks)}\n",
        "\n",
        "    for item in eval_dataset:\n",
        "        question = item[\"question\"]\n",
        "        ground_truth_docs_text = item[\"ground_truth_docs_text\"]\n",
        "\n",
        "        # 1. Get retrieved documents\n",
        "        retrieved_docs = retriever.invoke(question)\n",
        "        retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
        "\n",
        "        # 2. Determine relevance\n",
        "        # A retrieved doc is \"relevant\" if its text is in our ground truth list\n",
        "        relevance_list = [1 if text in ground_truth_docs_text else 0 for text in retrieved_texts]\n",
        "\n",
        "        # 3. Calculate Precision@k\n",
        "        precision_at_k = sum(relevance_list) / k if k > 0 else 0\n",
        "\n",
        "        # 4. Calculate Recall@k\n",
        "        num_relevant_docs = len(ground_truth_docs_text)\n",
        "        recall_at_k = sum(relevance_list) / num_relevant_docs if num_relevant_docs > 0 else 0\n",
        "\n",
        "        # 5. Calculate NDCG\n",
        "        # DCG and NDCG are designed for graded relevance, but we can use binary relevance (1 or 0)\n",
        "        # Reshape for sklearn\n",
        "        true_relevance = np.asarray([relevance_list])\n",
        "        ndcg_at_k = ndcg_score(true_relevance, true_relevance)\n",
        "\n",
        "        retrieval_results.append({\n",
        "            \"question\": question,\n",
        "            f\"Precision@{k}\": precision_at_k,\n",
        "            f\"Recall@{k}\": recall_at_k,\n",
        "            \"NDCG\": ndcg_at_k\n",
        "        })\n",
        "\n",
        "    avg_precision = np.mean([res[f\"Precision@{k}\"] for res in retrieval_results])\n",
        "    avg_recall = np.mean([res[f\"Recall@{k}\"] for res in retrieval_results])\n",
        "    avg_ndcg = np.mean([res[\"NDCG\"] for res in retrieval_results])\n",
        "\n",
        "    print(f\"Average Precision@{k}: {avg_precision:.4f}\")\n",
        "    print(f\"Average Recall@{k}: {avg_recall:.4f}\")\n",
        "    print(f\"Average NDCG: {avg_ndcg:.4f}\")\n",
        "    return retrieval_results"
      ],
      "metadata": {
        "id": "HcTZLcy95a3y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZfK3VthAhzQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f64190f7ea64a47a88c464f6f423127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb360d7f740643de9decc50ff1e76934",
              "IPY_MODEL_eed99ee3b4f94c12ac93f351a68b6157",
              "IPY_MODEL_719e8d9a15424409b8f48bbffd076594"
            ],
            "layout": "IPY_MODEL_46b6be703ec148b28af951cde405eb9d"
          }
        },
        "fb360d7f740643de9decc50ff1e76934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a38cb92c7c5b49cc9fe6d9ba269f02e0",
            "placeholder": "​",
            "style": "IPY_MODEL_c0425d8f98a646e9ba60472e8650ee54",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "eed99ee3b4f94c12ac93f351a68b6157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91148186172d4ccdafc6f41630551829",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bd9cd048ec048d78c497a6ac5adcc5c",
            "value": 8
          }
        },
        "719e8d9a15424409b8f48bbffd076594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c768fad76414fef968eb13db0508f06",
            "placeholder": "​",
            "style": "IPY_MODEL_9b1ea47fca284bf4a3bb76551276b604",
            "value": " 8/8 [01:18&lt;00:00,  7.96s/it]"
          }
        },
        "46b6be703ec148b28af951cde405eb9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a38cb92c7c5b49cc9fe6d9ba269f02e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0425d8f98a646e9ba60472e8650ee54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91148186172d4ccdafc6f41630551829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bd9cd048ec048d78c497a6ac5adcc5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c768fad76414fef968eb13db0508f06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b1ea47fca284bf4a3bb76551276b604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}